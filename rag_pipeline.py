import os
import pdfplumber
from markdownify import markdownify
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from sentence_transformers import SentenceTransformer
import numpy as np

# ğŸ“Œ Chemin du dossier contenant les PDFs dans le repo
DATA_DIR = "data"
MD_DIR = "markdown_files"
DB_DIR = "chroma_db"
RESULTS_FILE = "rÃ©sultats.md"

# ğŸ“Œ ModÃ¨les Hugging Face
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"  # ğŸ“Œ ModÃ¨le dâ€™embedding
GEN_MODEL = "mistralai/Mistral-7B-Instruct"  # ğŸ“Œ ModÃ¨le pour la gÃ©nÃ©ration

# ğŸ“Œ Liste de questions
QUESTIONS = [
    "Qu'est-ce qu'une fonction en Python ?",
    "Comment utiliser une boucle for ?",
    "Quelle est la diffÃ©rence entre while et for ?",
    "Comment gÃ©rer les erreurs avec try/except ?",
    "Qu'est-ce qu'une opÃ©ration CRUD en base de donnÃ©es avec Python ?"
]

# ğŸ“Œ Chargement du modÃ¨le dâ€™embedding
print("ğŸ” Chargement du modÃ¨le d'embedding...")
embedding_model = SentenceTransformer(EMBEDDING_MODEL)

def embed_texts(texts):
    """Transforme une liste de textes en vecteurs dâ€™embedding"""
    return embedding_model.encode(texts, convert_to_numpy=True)

# ğŸ“Œ Chargement du modÃ¨le Hugging Face pour la gÃ©nÃ©ration
print("ğŸš€ Chargement du modÃ¨le de gÃ©nÃ©ration...")
tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL)
model = AutoModelForCausalLM.from_pretrained(GEN_MODEL, torch_dtype=torch.float16, device_map="auto")
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# ğŸ“Œ CrÃ©ation des dossiers si nÃ©cessaire
os.makedirs(MD_DIR, exist_ok=True)
os.makedirs(DB_DIR, exist_ok=True)

# ğŸ“Œ Conversion PDF â†’ Markdown
def convert_pdf_to_markdown(pdf_path, md_path):
    """Extrait le texte d'un PDF et le convertit en Markdown."""
    md_text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                md_text += markdownify(text) + "\n\n"
    with open(md_path, "w", encoding="utf-8") as md_file:
        md_file.write(md_text)

# ğŸ“Œ Chargement des documents
def load_documents():
    """Charge les fichiers Markdown, en les gÃ©nÃ©rant depuis les PDF si nÃ©cessaire"""
    docs = []
    for filename in os.listdir(DATA_DIR):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(DATA_DIR, filename)
            md_filename = filename.replace(".pdf", ".md")
            md_path = os.path.join(MD_DIR, md_filename)
            if not os.path.exists(md_path):
                print(f"ğŸ“ Conversion du PDF en Markdown : {filename}")
                convert_pdf_to_markdown(pdf_path, md_path)
            loader = TextLoader(md_path)
            docs.extend(loader.load())
    return docs

# ğŸ“Œ CrÃ©ation de la base de vecteurs avec ChromaDB
def create_vectorstore(chunks):
    """CrÃ©e et stocke les embeddings dans ChromaDB"""
    print("ğŸ”„ GÃ©nÃ©ration des embeddings et stockage dans ChromaDB...")
    texts = [chunk.page_content for chunk in chunks]
    embeddings = embed_texts(texts)  # ğŸ“Œ GÃ©nÃ¨re les vecteurs dâ€™embedding
    vectorstore = Chroma.from_embeddings(texts, embeddings, persist_directory=DB_DIR)
    return vectorstore

# ğŸ“Œ Recherche des informations dans Chroma
def retrieve_context(query, k=3, char_limit=1000):
    """Recherche les passages pertinents pour rÃ©pondre Ã  une question"""
    vectorstore = Chroma(persist_directory=DB_DIR)
    query_embedding = embed_texts([query])  # ğŸ“Œ Embedding de la question
    results = vectorstore.similarity_search_by_vector(query_embedding[0], k=k)
    context_list = list(dict.fromkeys([res.page_content.strip() for res in results]))
    context = "\n".join(context_list)
    return context if len(context) <= char_limit else context[:char_limit]

# ğŸ“Œ GÃ©nÃ©ration de rÃ©ponse
def generate_response(query):
    """Construit le prompt et gÃ©nÃ¨re une rÃ©ponse avec Hugging Face"""
    context = retrieve_context(query)
    if not context:
        return "âŒ Aucun contexte pertinent trouvÃ©."
    prompt = f"""
    ğŸ“š **Contexte** :
    {context}

    â“ **Question** : {query}
    """
    response = generator(prompt, max_length=500, temperature=0.7, do_sample=True)
    return response[0]["generated_text"]

# ğŸ“Œ ExÃ©cution automatique des questions
def main():
    print("\nğŸ“¥ Chargement des documents...")
    documents = load_documents()
    if not documents:
        print("âš ï¸ Aucun document trouvÃ© dans 'data/'. Ajoutez des PDF et rÃ©essayez.")
        return
    chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100).split_documents(documents)
    create_vectorstore(chunks)
    print("âœ… Base vectorielle prÃªte.")

    for question in QUESTIONS:
        response = generate_response(question)
        print(f"\nâ“ {question.upper()}\nğŸ¤– {response}\n")

if __name__ == "__main__":
    main()