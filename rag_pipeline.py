import os
import pdfplumber
from markdownify import markdownify
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

# ğŸ“Œ Configuration des chemins
DATA_DIR = "data"
MD_DIR = "markdown_files"
DB_DIR = "chroma_db"
RESULTS_FILE = "rÃ©sultats.md"

# ğŸ“Œ ModÃ¨le dâ€™embedding Hugging Face
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# ğŸ“Œ ModÃ¨le Hugging Face pour la gÃ©nÃ©ration
GEN_MODEL = "mistralai/Mistral-7B-v0.3"

# ğŸ“Œ Liste de questions Ã  poser automatiquement
QUESTIONS = [
    "Qu'est-ce qu'une fonction en Python ?",
    "Comment utiliser une boucle for ?",
    "Quelle est la diffÃ©rence entre while et for ?",
    "Comment gÃ©rer les erreurs avec try/except ?",
    "Qu'est-ce qu'une opÃ©ration CRUD en base de donnÃ©es avec Python ?"
]

# ğŸ“Œ Chargement du modÃ¨le dâ€™embedding
print("ğŸ” Chargement du modÃ¨le d'embedding...")
embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

# ğŸ“Œ Chargement du modÃ¨le Hugging Face pour la gÃ©nÃ©ration
print("ğŸš€ Chargement du modÃ¨le de gÃ©nÃ©ration...")
tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL)
model = AutoModelForCausalLM.from_pretrained(
    GEN_MODEL,
    torch_dtype=torch.float16,
    device_map="auto"
)
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# ğŸ“Œ CrÃ©ation des dossiers si nÃ©cessaire
os.makedirs(MD_DIR, exist_ok=True)
os.makedirs(DB_DIR, exist_ok=True)

# ğŸ“Œ Conversion PDF â†’ Markdown
def convert_pdf_to_markdown(pdf_path, md_path):
    """Extrait le texte d'un PDF et le convertit en Markdown."""
    md_text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                md_text += markdownify(text) + "\n\n"
    with open(md_path, "w", encoding="utf-8") as md_file:
        md_file.write(md_text)

# ğŸ“Œ Chargement des documents
def load_documents():
    """Charge les fichiers Markdown, en les gÃ©nÃ©rant depuis les PDF si nÃ©cessaire"""
    docs = []
    for filename in os.listdir(DATA_DIR):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(DATA_DIR, filename)
            md_filename = filename.replace(".pdf", ".md")
            md_path = os.path.join(MD_DIR, md_filename)

            # âœ… Convertir PDF en Markdown si nÃ©cessaire
            if not os.path.exists(md_path):
                print(f"ğŸ“ Conversion du PDF en Markdown : {filename}")
                convert_pdf_to_markdown(pdf_path, md_path)

            # âœ… Chargement du fichier Markdown avec encodage UTF-8
            loader = TextLoader(md_path, encoding="utf-8")
            docs.extend(loader.load())

    return docs

# ğŸ“Œ CrÃ©ation de la base de vecteurs avec ChromaDB
def create_vectorstore(chunks):
    """CrÃ©e et stocke les embeddings dans ChromaDB"""
    print("ğŸ”„ GÃ©nÃ©ration des embeddings et stockage dans ChromaDB...")
    vectorstore = Chroma.from_documents(chunks, embedding_model, persist_directory=DB_DIR)
    return vectorstore

# ğŸ“Œ Recherche des informations dans Chroma
def retrieve_context(query, k=3, char_limit=1000):
    """Recherche les passages pertinents dans ChromaDB"""
    vectorstore = Chroma(persist_directory=DB_DIR, embedding_function=embedding_model)
    results = vectorstore.similarity_search(query, k=k)

    context_list = list(dict.fromkeys([res.page_content.strip() for res in results]))
    context = "\n".join(context_list)

    return context if len(context) <= char_limit else context[:char_limit]

# ğŸ“Œ GÃ©nÃ©ration de rÃ©ponse avec Hugging Face
def generate_response(query):
    """Construit le prompt et gÃ©nÃ¨re une rÃ©ponse avec Hugging Face"""
    context = retrieve_context(query)

    if not context:
        return "âŒ Aucun contexte pertinent trouvÃ©."

    # ğŸ“Œ Construction du prompt
    prompt = f"""
    ğŸ“š **Contexte** :
    {context}

    â“ **Question** : {query}
    """

    # ğŸ“Œ GÃ©nÃ©ration de texte avec Hugging Face
    response = generator(prompt, max_length=500, temperature=0.7, do_sample=True)
    return response[0]["generated_text"]

# ğŸ“Œ Enregistrement des rÃ©ponses dans un fichier Markdown
def save_to_file(question, response):
    """Enregistre les rÃ©ponses dans un fichier Markdown"""
    with open(RESULTS_FILE, "a", encoding="utf-8") as f:
        f.write(f"\n## â“ {question.upper()}\n\n")
        f.write(f"{response}\n")
        f.write("=" * 100 + "\n")

# ğŸ“Œ ExÃ©cution automatique des questions
def main():
    print("\nğŸ“¥ Chargement des documents...")
    documents = load_documents()
    if not documents:
        print("âš ï¸ Aucun document trouvÃ© dans 'data/'. Ajoutez des PDF et rÃ©essayez.")
        return
    chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100).split_documents(documents)
    create_vectorstore(chunks)
    print("âœ… Base vectorielle prÃªte.")

    for question in QUESTIONS:
        response = generate_response(question)
        print(f"\nâ“ {question.upper()}\nğŸ¤– {response}\n")
        save_to_file(question, response)

if __name__ == "__main__":
    main()
