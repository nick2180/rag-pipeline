import os
import pdfplumber
from markdownify import markdownify
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import ollama

# ğŸ›  Configuration
DATA_DIR = "data"  # Dossier contenant les PDF
MD_DIR = "markdown_files"  # Dossier oÃ¹ stocker les fichiers Markdown
DB_DIR = "chroma_db"  # Base de donnÃ©es ChromaDB
EMBED_MODEL = "nomic-embed-text"  # ModÃ¨le d'embedding utilisÃ© par Ollama
GEN_MODEL = "mistral"  # ModÃ¨le pour la gÃ©nÃ©ration de rÃ©ponses
RESULTS_FILE = "rÃ©sultats.md"  # Fichier oÃ¹ enregistrer les rÃ©sultats

# Liste de questions prÃ©dÃ©finies ğŸ“Œ
QUESTIONS = [
    "Qu'est-ce qu'une fonction en Python ?",
    "Comment utiliser une boucle for en Python ?",
    "Qu'est-ce qu'une condition if-else en Python ?",
    "Comment utiliser le module random en Python ?",
    "Qu'est-ce qu'une opÃ©ration CRUD en base de donnÃ©es avec Python ?",
    "Comment utiliser une liste en Python ?",
	"Quelle est la diffÃ©rence entre while et for ?",
	"Comment gÃ©rer les erreurs avec try/except ?"
]

# CrÃ©ation des dossiers si nÃ©cessaire
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(MD_DIR, exist_ok=True)
os.makedirs(DB_DIR, exist_ok=True)

# ğŸ“Œ CONVERSION PDF -> MARKDOWN
def convert_pdf_to_markdown(pdf_path, md_path):
    """Extrait le texte d'un PDF et le convertit en Markdown."""
    md_text = ""
    
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                md_text += markdownify(text) + "\n\n"  # Convertir en Markdown
    
    with open(md_path, "w", encoding="utf-8") as md_file:
        md_file.write(md_text)

# ğŸ“Œ CHARGEMENT DES DOCUMENTS
def load_documents(data_dir, md_dir):
    """Charge les fichiers Markdown, en les gÃ©nÃ©rant depuis les PDF si nÃ©cessaire"""
    docs = []

    for filename in os.listdir(data_dir):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(data_dir, filename)
            md_filename = filename.replace(".pdf", ".md")
            md_path = os.path.join(md_dir, md_filename)

            # Convertir le PDF en Markdown s'il n'existe pas encore
            if not os.path.exists(md_path):
                print(f"ğŸ“ Conversion du PDF en Markdown : {filename}")
                convert_pdf_to_markdown(pdf_path, md_path)

            # Charger le fichier Markdown
            loader = TextLoader(md_path)
            docs.extend(loader.load())

    return docs

# ğŸ“Œ TRAITEMENT : DÃ‰COUPE EN CHUNKS
def process_documents(docs):
    """Divise les documents en morceaux pour l'indexation"""
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    return text_splitter.split_documents(docs)

# ğŸ“Œ STOCKAGE DES EMBEDDINGS DANS CHROMA
def create_vectorstore(chunks):
    """CrÃ©e et stocke les embeddings dans ChromaDB"""
    print("ğŸ”„ GÃ©nÃ©ration des embeddings et stockage dans ChromaDB...")
    embeddings = OllamaEmbeddings(model=EMBED_MODEL)
    vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=DB_DIR)
    return vectorstore

# ğŸ“Œ RECHERCHE DES INFORMATIONS DANS CHROMA
def retrieve_context(query, min_k=2, max_k=5, char_limit=1000):
    """Recherche les passages pertinents en ajustant dynamiquement k"""
    embeddings = OllamaEmbeddings(model=EMBED_MODEL)
    vectorstore = Chroma(persist_directory=DB_DIR, embedding_function=embeddings)

    for k in range(min_k, max_k+1):  # Essai avec diffÃ©rents k
        results = vectorstore.max_marginal_relevance_search(query, k=k, fetch_k=10, lambda_mult=0.5)
        
        # Supprime les doublons tout en conservant l'ordre
        context_list = list(dict.fromkeys([res.page_content.strip() for res in results]))
        context = "\n".join(context_list)

        if len(context) > char_limit:
            break  # ArrÃªte la rÃ©cupÃ©ration si trop de texte est ajoutÃ©
    
    print("\nğŸ“œ Contexte utilisÃ© pour la rÃ©ponse :\n", context)  # DÃ©bogage
    return context

# ğŸ“Œ GÃ‰NÃ‰RATION DE RÃ‰PONSE AVEC OLLAMA
def generate_response(query):
    """Construit le prompt et gÃ©nÃ¨re la rÃ©ponse avec Ollama"""
    context = retrieve_context(query)

    if not context:
        return "âŒ Aucun contexte pertinent trouvÃ©. VÃ©rifiez vos cours."

    prompt = f"""
Tu es un assistant qui **doit uniquement retranscrire le contenu des cours**.

ğŸš¨ **RÃ¨gles strictes** :
1. **Ne jamais inventer** ou complÃ©ter les informations.
2. **Ne rÃ©pondre quâ€™avec des extraits exacts** du cours.
3. Si l'information demandÃ©e **n'est pas dans le contexte**, rÃ©pondre :
   **"Je ne sais pas. VÃ©rifiez vos cours."**

ğŸ“š **Extrait du cours** :
{context}

â“ **Question de l'Ã©lÃ¨ve** : {query}

ğŸ’¡ **RÃ©ponds en formatant clairement la rÃ©ponse :**
- **Titre de la section concernÃ©e**
- **Extrait exact du cours**
- **Exemple de code (si applicable)**
- **Donne la sortie du code (si applicable)**
"""
    response = ollama.chat(model=GEN_MODEL, messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

# ğŸ“Œ ENREGISTREMENT DES RÃ‰PONSES DANS UN FICHIER MARKDOWN
def save_to_file(question, response):
    """Enregistre les rÃ©ponses dans un fichier Markdown"""
    with open(RESULTS_FILE, "a", encoding="utf-8") as f:
        f.write(f"\n## â“ {question.upper()}\n\n")
        f.write(f"{response}\n")
        f.write("=" * 100 + "\n")

# ğŸ“Œ EXÃ‰CUTION DU PIPELINE
def main():
    """ExÃ©cute toutes les Ã©tapes du RAG"""
    print("\nğŸ“¥ Chargement des documents...")
    documents = load_documents(DATA_DIR, MD_DIR)

    if not documents:
        print("âš ï¸ Aucun document trouvÃ© dans 'data/'. Ajoutez des PDF et rÃ©essayez.")
        return

    print(f"ğŸ“ƒ {len(documents)} documents trouvÃ©s. DÃ©coupage en chunks...")
    chunks = process_documents(documents)

    print(f"ğŸ” {len(chunks)} chunks gÃ©nÃ©rÃ©s. CrÃ©ation de la base de vecteurs...")
    create_vectorstore(chunks)
    print("âœ… Base de donnÃ©es vectorielle crÃ©Ã©e et sauvegardÃ©e.")

    print("\nğŸ’¡ **DÃ‰BUT DES QUESTIONS AUTOMATIQUES** ğŸ’¡\n")
    for question in QUESTIONS:
        print("=" * 100)
        print(f"\nâ“ **QUESTION** : {question.upper()}\n")
        response = generate_response(question)
        print(f"\nğŸ¤– **RÃ‰PONSE Dâ€™OLLAMA** :\n{response}\n")
        save_to_file(question, response)

if __name__ == "__main__":
    main()